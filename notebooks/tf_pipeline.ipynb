{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 01:02:14.269558: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from ds.data.sets import load_sets_v2\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = joblib.load(\"../data/processed/datasetsjmone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = [\n",
    "    \"review_overall\",\n",
    "    \"beer_name\",\n",
    "]\n",
    "for data in datasets:\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        for column in drops:\n",
    "            if column in data.columns:\n",
    "                data.drop(columns=column, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, x_validation, y_validation = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brewery_name</th>\n",
       "      <th>review_aroma</th>\n",
       "      <th>review_appearance</th>\n",
       "      <th>review_palate</th>\n",
       "      <th>review_taste</th>\n",
       "      <th>beer_abv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>590715</th>\n",
       "      <td>goose island beer co</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514329</th>\n",
       "      <td>hinterland brewery restaurant</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272592</th>\n",
       "      <td>flying dog brewery</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894213</th>\n",
       "      <td>stone brewing co</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174214</th>\n",
       "      <td>long trail brewing co</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759715</th>\n",
       "      <td>widmer brothers brewing company</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618842</th>\n",
       "      <td>harpoon brewery</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948973</th>\n",
       "      <td>bells brewery inc</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678833</th>\n",
       "      <td>moorhouses brewery burnley ltd</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120927</th>\n",
       "      <td>genesee brewing co / dundee brewing co</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1361552 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   brewery_name  review_aroma  \\\n",
       "590715                     goose island beer co           5.0   \n",
       "1514329           hinterland brewery restaurant           4.0   \n",
       "272592                       flying dog brewery           3.5   \n",
       "894213                         stone brewing co           4.0   \n",
       "174214                    long trail brewing co           4.0   \n",
       "...                                         ...           ...   \n",
       "759715          widmer brothers brewing company           3.5   \n",
       "618842                          harpoon brewery           3.5   \n",
       "948973                        bells brewery inc           3.5   \n",
       "678833           moorhouses brewery burnley ltd           3.5   \n",
       "120927   genesee brewing co / dundee brewing co           3.0   \n",
       "\n",
       "         review_appearance  review_palate  review_taste  beer_abv  \n",
       "590715                 4.5            4.0           5.0     13.00  \n",
       "1514329                4.0            3.5           3.5      6.85  \n",
       "272592                 3.0            3.5           3.5      5.50  \n",
       "894213                 4.5            4.0           4.0      7.20  \n",
       "174214                 4.0            4.0           4.0      8.60  \n",
       "...                    ...            ...           ...       ...  \n",
       "759715                 4.5            3.0           3.5      9.40  \n",
       "618842                 3.5            3.5           3.5      4.30  \n",
       "948973                 3.5            3.5           3.5      5.20  \n",
       "678833                 4.0            3.0           3.5      3.40  \n",
       "120927                 4.0            3.5           3.5      4.62  \n",
       "\n",
       "[1361552 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.fillna(-9)\n",
    "x_test = x_test.fillna(-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_train\n",
    "y_vocab = sorted(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 01:02:16.457127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-11 01:02:16.459855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-11 01:02:16.459990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-11 01:02:16.460934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-11 01:02:16.461067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-11 01:02:16.461185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-11 01:02:16.844299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-11 01:02:16.844446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-11 01:02:16.844560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-11 01:02:16.844660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4809 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:0b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "y_tensor = tf.convert_to_tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lookup = tf.keras.layers.StringLookup(vocabulary=y_vocab, output_mode='one_hot')\n",
    "target = target_lookup(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((dict(df), target))\n",
    "ds = ds.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tensor = tf.convert_to_tensor(y_test)\n",
    "y_test_encoded = target_lookup(y_test_tensor)\n",
    "test = tf.data.Dataset.from_tensor_slices((dict(x_test), y_test_encoded))\n",
    "test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #compute imputer mean\n",
    "# mask = tf.where(tf.math.is_nan(tensor) , 0. , tensor)\n",
    "# mask_norm = tf.reduce_sum(tf.clip_by_value(mask, 0., 1.),axis=0)\n",
    "# imp_mean = tf.math.divide(tf.reduce_sum(mask, axis=0), mask_norm)\n",
    "\n",
    "# #transform\n",
    "# tf.where(tf.math.is_nan(X) , imp_mean , X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_names = [\n",
    "    # \"review_overall\",\n",
    "    \"review_aroma\",\n",
    "    \"review_appearance\",\n",
    "    \"review_palate\",\n",
    "    \"review_taste\",\n",
    "]\n",
    "numeric_features = df[numeric_feature_names]\n",
    "\n",
    "categorical_feature_names = [\n",
    "    \"brewery_name\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_dict(inputs, fun=tf.stack):\n",
    "    values = []\n",
    "    for key in sorted(inputs.keys()):\n",
    "      values.append(tf.cast(inputs[key], tf.float32))\n",
    "\n",
    "    return fun(values, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "preprocessed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, column in x_train.items():\n",
    "  if type(column[0]) == str:\n",
    "    dtype = tf.string\n",
    "  else:\n",
    "    dtype = tf.float32\n",
    "\n",
    "  inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(stack_dict(dict(numeric_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'normalization')>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_inputs = {}\n",
    "for name in numeric_feature_names:\n",
    "  numeric_inputs[name]=inputs[name]\n",
    "\n",
    "numeric_inputs = stack_dict(numeric_inputs)\n",
    "numeric_normalized = normalizer(numeric_inputs)\n",
    "\n",
    "preprocessed.append(numeric_normalized)\n",
    "\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in categorical_feature_names:\n",
    "  vocab = sorted(set(df[name]))\n",
    "\n",
    "  if type(vocab[0]) is str:\n",
    "    lookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "  else:\n",
    "    lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "\n",
    "  x = inputs[name][:, tf.newaxis]\n",
    "  x = lookup(x)\n",
    "  preprocessed.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "preprocesssed_result = tf.concat(preprocessed, axis=-1)\n",
    "preprocessor = tf.keras.Model(inputs, preprocesssed_result)\n",
    "tf.keras.utils.plot_model(preprocessor, rankdir=\"LR\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.dense1= tf.keras.layers.Dense(4096, activation='relu')\n",
    "    self.dense2 = tf.keras.layers.Dense(2048, activation='relu')\n",
    "    self.dense3 = tf.keras.layers.Dense(4096, activation='relu')\n",
    "    self.dense4 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "    self.dense5 = tf.keras.layers.Dense(2048, activation='relu')\n",
    "    self.attn1 = tf.keras.layers.Attention()\n",
    "    self.attn2 = tf.keras.layers.Attention()\n",
    "    self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "    self.outputlyr = tf.keras.layers.Dense(target_lookup.vocabulary_size(), activation='softmax')\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.mask = tf.keras.layers.Masking(mask_value=0.0)\n",
    "    \n",
    "  def call(self, inputs, training=False):\n",
    "    # x = self.flatten(inputs)\n",
    "    x = self.dense1(inputs)\n",
    "    # x = self.attn1(([x, x]))\n",
    "    x = self.dense2(x)\n",
    "    x = self.dense3(x)\n",
    "\n",
    "    # if training:\n",
    "    #   x = self.dropout(x, training=training)\n",
    "    x = self.dense4(x)\n",
    "    # x = self.dense5(x)\n",
    "\n",
    "    return self.outputlyr(x)\n",
    "\n",
    "body = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body = tf.keras.Sequential([\n",
    "#   tf.keras.layers.Dense(512, activation='relu'),\n",
    "#   tf.keras.layers.Dense(256, activation='relu'),\n",
    "#   tf.keras.layers.Dense(32, activation='relu'),\n",
    "#   tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dense(32, activation='relu'),\n",
    "#   tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dense(target_lookup.vocabulary_size(), activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 5862) dtype=float32 (created by layer 'model')>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = preprocessor(inputs)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 105) dtype=float32 (created by layer 'my_model_20')>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = body(x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs, result)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "137/137 [==============================] - 37s 264ms/step - loss: 3.0738 - accuracy: 0.2246 - val_loss: 2.4516 - val_accuracy: 0.3069\n",
      "Epoch 2/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.3250 - accuracy: 0.3163 - val_loss: 2.3084 - val_accuracy: 0.3111\n",
      "Epoch 3/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.2261 - accuracy: 0.3240 - val_loss: 2.2650 - val_accuracy: 0.3154\n",
      "Epoch 4/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.1810 - accuracy: 0.3294 - val_loss: 2.2664 - val_accuracy: 0.3129\n",
      "Epoch 5/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.1538 - accuracy: 0.3333 - val_loss: 2.2554 - val_accuracy: 0.3152\n",
      "Epoch 6/200\n",
      "137/137 [==============================] - 36s 264ms/step - loss: 2.1321 - accuracy: 0.3369 - val_loss: 2.2554 - val_accuracy: 0.3176\n",
      "Epoch 7/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.1133 - accuracy: 0.3406 - val_loss: 2.2665 - val_accuracy: 0.3173\n",
      "Epoch 8/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.0980 - accuracy: 0.3433 - val_loss: 2.2784 - val_accuracy: 0.3171\n",
      "Epoch 9/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.0856 - accuracy: 0.3460 - val_loss: 2.2960 - val_accuracy: 0.3148\n",
      "Epoch 10/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.0752 - accuracy: 0.3482 - val_loss: 2.3121 - val_accuracy: 0.3168\n",
      "Epoch 11/200\n",
      "137/137 [==============================] - 36s 264ms/step - loss: 2.0652 - accuracy: 0.3504 - val_loss: 2.3412 - val_accuracy: 0.3168\n",
      "Epoch 12/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.0547 - accuracy: 0.3523 - val_loss: 2.3575 - val_accuracy: 0.3149\n",
      "Epoch 13/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.0440 - accuracy: 0.3546 - val_loss: 2.3640 - val_accuracy: 0.3135\n",
      "Epoch 14/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.0322 - accuracy: 0.3574 - val_loss: 2.3570 - val_accuracy: 0.3143\n",
      "Epoch 15/200\n",
      "137/137 [==============================] - 36s 264ms/step - loss: 2.0215 - accuracy: 0.3599 - val_loss: 2.3742 - val_accuracy: 0.3159\n",
      "Epoch 16/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.0122 - accuracy: 0.3621 - val_loss: 2.4040 - val_accuracy: 0.3144\n",
      "Epoch 17/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 2.0047 - accuracy: 0.3634 - val_loss: 2.4232 - val_accuracy: 0.3127\n",
      "Epoch 18/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9947 - accuracy: 0.3663 - val_loss: 2.4354 - val_accuracy: 0.3146\n",
      "Epoch 19/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9837 - accuracy: 0.3691 - val_loss: 2.4649 - val_accuracy: 0.3150\n",
      "Epoch 20/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9735 - accuracy: 0.3716 - val_loss: 2.4794 - val_accuracy: 0.3140\n",
      "Epoch 21/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9657 - accuracy: 0.3730 - val_loss: 2.5012 - val_accuracy: 0.3139\n",
      "Epoch 22/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9565 - accuracy: 0.3759 - val_loss: 2.5233 - val_accuracy: 0.3121\n",
      "Epoch 23/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9460 - accuracy: 0.3786 - val_loss: 2.5126 - val_accuracy: 0.3129\n",
      "Epoch 24/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9339 - accuracy: 0.3818 - val_loss: 2.5441 - val_accuracy: 0.3088\n",
      "Epoch 25/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9270 - accuracy: 0.3836 - val_loss: 2.5263 - val_accuracy: 0.3108\n",
      "Epoch 26/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9205 - accuracy: 0.3854 - val_loss: 2.5629 - val_accuracy: 0.3074\n",
      "Epoch 27/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.9109 - accuracy: 0.3877 - val_loss: 2.5823 - val_accuracy: 0.3053\n",
      "Epoch 28/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.9008 - accuracy: 0.3903 - val_loss: 2.6239 - val_accuracy: 0.3043\n",
      "Epoch 29/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8915 - accuracy: 0.3927 - val_loss: 2.5974 - val_accuracy: 0.3069\n",
      "Epoch 30/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8823 - accuracy: 0.3952 - val_loss: 2.6097 - val_accuracy: 0.3057\n",
      "Epoch 31/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8755 - accuracy: 0.3973 - val_loss: 2.6267 - val_accuracy: 0.3062\n",
      "Epoch 32/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8674 - accuracy: 0.3994 - val_loss: 2.6555 - val_accuracy: 0.3034\n",
      "Epoch 33/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.8584 - accuracy: 0.4021 - val_loss: 2.6432 - val_accuracy: 0.3017\n",
      "Epoch 34/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8496 - accuracy: 0.4043 - val_loss: 2.7307 - val_accuracy: 0.3004\n",
      "Epoch 35/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8423 - accuracy: 0.4061 - val_loss: 2.7778 - val_accuracy: 0.3025\n",
      "Epoch 36/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8344 - accuracy: 0.4081 - val_loss: 2.7753 - val_accuracy: 0.3021\n",
      "Epoch 37/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8272 - accuracy: 0.4098 - val_loss: 2.7984 - val_accuracy: 0.3002\n",
      "Epoch 38/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.8196 - accuracy: 0.4121 - val_loss: 2.7925 - val_accuracy: 0.2998\n",
      "Epoch 39/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8121 - accuracy: 0.4143 - val_loss: 2.7844 - val_accuracy: 0.3002\n",
      "Epoch 40/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.8023 - accuracy: 0.4171 - val_loss: 2.8080 - val_accuracy: 0.2993\n",
      "Epoch 41/200\n",
      "137/137 [==============================] - 36s 264ms/step - loss: 1.7932 - accuracy: 0.4194 - val_loss: 2.8330 - val_accuracy: 0.2979\n",
      "Epoch 42/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.7877 - accuracy: 0.4207 - val_loss: 2.8490 - val_accuracy: 0.2981\n",
      "Epoch 43/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.7802 - accuracy: 0.4229 - val_loss: 2.8707 - val_accuracy: 0.2971\n",
      "Epoch 44/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.7700 - accuracy: 0.4260 - val_loss: 2.9335 - val_accuracy: 0.2964\n",
      "Epoch 45/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.7618 - accuracy: 0.4279 - val_loss: 3.0116 - val_accuracy: 0.2938\n",
      "Epoch 46/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.7580 - accuracy: 0.4285 - val_loss: 2.9845 - val_accuracy: 0.2951\n",
      "Epoch 47/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.7531 - accuracy: 0.4302 - val_loss: 3.0027 - val_accuracy: 0.2923\n",
      "Epoch 48/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.7472 - accuracy: 0.4318 - val_loss: 3.0459 - val_accuracy: 0.2901\n",
      "Epoch 49/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.7404 - accuracy: 0.4337 - val_loss: 3.0894 - val_accuracy: 0.2906\n",
      "Epoch 50/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.7332 - accuracy: 0.4353 - val_loss: 3.1174 - val_accuracy: 0.2912\n",
      "Epoch 51/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.7274 - accuracy: 0.4372 - val_loss: 3.0617 - val_accuracy: 0.2902\n",
      "Epoch 52/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.7216 - accuracy: 0.4386 - val_loss: 3.1074 - val_accuracy: 0.2896\n",
      "Epoch 53/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.7156 - accuracy: 0.4404 - val_loss: 3.1529 - val_accuracy: 0.2907\n",
      "Epoch 54/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.7057 - accuracy: 0.4436 - val_loss: 3.1992 - val_accuracy: 0.2907\n",
      "Epoch 55/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6960 - accuracy: 0.4464 - val_loss: 3.2178 - val_accuracy: 0.2885\n",
      "Epoch 56/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6915 - accuracy: 0.4472 - val_loss: 3.2126 - val_accuracy: 0.2875\n",
      "Epoch 57/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6860 - accuracy: 0.4488 - val_loss: 3.2024 - val_accuracy: 0.2865\n",
      "Epoch 58/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6815 - accuracy: 0.4507 - val_loss: 3.2077 - val_accuracy: 0.2867\n",
      "Epoch 59/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6738 - accuracy: 0.4525 - val_loss: 3.2602 - val_accuracy: 0.2864\n",
      "Epoch 60/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.6686 - accuracy: 0.4541 - val_loss: 3.2205 - val_accuracy: 0.2863\n",
      "Epoch 61/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6597 - accuracy: 0.4566 - val_loss: 3.1514 - val_accuracy: 0.2854\n",
      "Epoch 62/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6511 - accuracy: 0.4593 - val_loss: 3.1814 - val_accuracy: 0.2841\n",
      "Epoch 63/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.6471 - accuracy: 0.4598 - val_loss: 3.1970 - val_accuracy: 0.2838\n",
      "Epoch 64/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6383 - accuracy: 0.4623 - val_loss: 3.2334 - val_accuracy: 0.2829\n",
      "Epoch 65/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6298 - accuracy: 0.4645 - val_loss: 3.3096 - val_accuracy: 0.2822\n",
      "Epoch 66/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6214 - accuracy: 0.4670 - val_loss: 3.4036 - val_accuracy: 0.2820\n",
      "Epoch 67/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.6150 - accuracy: 0.4692 - val_loss: 3.4475 - val_accuracy: 0.2835\n",
      "Epoch 68/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6071 - accuracy: 0.4712 - val_loss: 3.5696 - val_accuracy: 0.2826\n",
      "Epoch 69/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.6009 - accuracy: 0.4731 - val_loss: 3.5433 - val_accuracy: 0.2818\n",
      "Epoch 70/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.5952 - accuracy: 0.4747 - val_loss: 3.5464 - val_accuracy: 0.2831\n",
      "Epoch 71/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5864 - accuracy: 0.4770 - val_loss: 3.5164 - val_accuracy: 0.2812\n",
      "Epoch 72/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5814 - accuracy: 0.4784 - val_loss: 3.5288 - val_accuracy: 0.2790\n",
      "Epoch 73/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5754 - accuracy: 0.4802 - val_loss: 3.5366 - val_accuracy: 0.2791\n",
      "Epoch 74/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5696 - accuracy: 0.4819 - val_loss: 3.5423 - val_accuracy: 0.2762\n",
      "Epoch 75/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5629 - accuracy: 0.4837 - val_loss: 3.4836 - val_accuracy: 0.2753\n",
      "Epoch 76/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5595 - accuracy: 0.4841 - val_loss: 3.4890 - val_accuracy: 0.2756\n",
      "Epoch 77/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5533 - accuracy: 0.4859 - val_loss: 3.5123 - val_accuracy: 0.2766\n",
      "Epoch 78/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5478 - accuracy: 0.4877 - val_loss: 3.6365 - val_accuracy: 0.2742\n",
      "Epoch 79/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5420 - accuracy: 0.4895 - val_loss: 3.7463 - val_accuracy: 0.2759\n",
      "Epoch 80/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.5379 - accuracy: 0.4902 - val_loss: 3.7606 - val_accuracy: 0.2780\n",
      "Epoch 81/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5332 - accuracy: 0.4918 - val_loss: 3.6638 - val_accuracy: 0.2760\n",
      "Epoch 82/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5278 - accuracy: 0.4929 - val_loss: 3.6536 - val_accuracy: 0.2732\n",
      "Epoch 83/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5235 - accuracy: 0.4945 - val_loss: 3.6595 - val_accuracy: 0.2733\n",
      "Epoch 84/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5165 - accuracy: 0.4963 - val_loss: 3.7107 - val_accuracy: 0.2745\n",
      "Epoch 85/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.5139 - accuracy: 0.4969 - val_loss: 3.9024 - val_accuracy: 0.2753\n",
      "Epoch 86/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5108 - accuracy: 0.4979 - val_loss: 4.0706 - val_accuracy: 0.2733\n",
      "Epoch 87/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5084 - accuracy: 0.4987 - val_loss: 4.0948 - val_accuracy: 0.2725\n",
      "Epoch 88/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.5055 - accuracy: 0.4991 - val_loss: 4.0862 - val_accuracy: 0.2721\n",
      "Epoch 89/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.5021 - accuracy: 0.4999 - val_loss: 4.1622 - val_accuracy: 0.2700\n",
      "Epoch 90/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4968 - accuracy: 0.5015 - val_loss: 4.1872 - val_accuracy: 0.2685\n",
      "Epoch 91/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4932 - accuracy: 0.5020 - val_loss: 4.1774 - val_accuracy: 0.2684\n",
      "Epoch 92/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4904 - accuracy: 0.5027 - val_loss: 4.2670 - val_accuracy: 0.2704\n",
      "Epoch 93/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4830 - accuracy: 0.5052 - val_loss: 4.3321 - val_accuracy: 0.2704\n",
      "Epoch 94/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4782 - accuracy: 0.5068 - val_loss: 4.4474 - val_accuracy: 0.2692\n",
      "Epoch 95/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4755 - accuracy: 0.5073 - val_loss: 4.4861 - val_accuracy: 0.2684\n",
      "Epoch 96/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4706 - accuracy: 0.5086 - val_loss: 4.4693 - val_accuracy: 0.2689\n",
      "Epoch 97/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4677 - accuracy: 0.5096 - val_loss: 4.4258 - val_accuracy: 0.2670\n",
      "Epoch 98/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4650 - accuracy: 0.5103 - val_loss: 4.4527 - val_accuracy: 0.2664\n",
      "Epoch 99/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4611 - accuracy: 0.5112 - val_loss: 4.5393 - val_accuracy: 0.2677\n",
      "Epoch 100/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4577 - accuracy: 0.5123 - val_loss: 4.5156 - val_accuracy: 0.2701\n",
      "Epoch 101/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4573 - accuracy: 0.5126 - val_loss: 4.4432 - val_accuracy: 0.2694\n",
      "Epoch 102/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4557 - accuracy: 0.5127 - val_loss: 4.4235 - val_accuracy: 0.2696\n",
      "Epoch 103/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4548 - accuracy: 0.5131 - val_loss: 4.4253 - val_accuracy: 0.2678\n",
      "Epoch 104/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4515 - accuracy: 0.5141 - val_loss: 4.5723 - val_accuracy: 0.2681\n",
      "Epoch 105/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4468 - accuracy: 0.5153 - val_loss: 4.5887 - val_accuracy: 0.2694\n",
      "Epoch 106/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4468 - accuracy: 0.5150 - val_loss: 4.5742 - val_accuracy: 0.2699\n",
      "Epoch 107/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4449 - accuracy: 0.5153 - val_loss: 4.6300 - val_accuracy: 0.2698\n",
      "Epoch 108/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4434 - accuracy: 0.5162 - val_loss: 4.6268 - val_accuracy: 0.2701\n",
      "Epoch 109/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4555 - accuracy: 0.5127 - val_loss: 4.5650 - val_accuracy: 0.2688\n",
      "Epoch 110/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4499 - accuracy: 0.5138 - val_loss: 4.5410 - val_accuracy: 0.2657\n",
      "Epoch 111/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4464 - accuracy: 0.5153 - val_loss: 4.6523 - val_accuracy: 0.2655\n",
      "Epoch 112/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4502 - accuracy: 0.5139 - val_loss: 4.7021 - val_accuracy: 0.2664\n",
      "Epoch 113/200\n",
      "137/137 [==============================] - 36s 261ms/step - loss: 1.4507 - accuracy: 0.5135 - val_loss: 4.6752 - val_accuracy: 0.2661\n",
      "Epoch 114/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4449 - accuracy: 0.5150 - val_loss: 4.5958 - val_accuracy: 0.2672\n",
      "Epoch 115/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4472 - accuracy: 0.5148 - val_loss: 4.5042 - val_accuracy: 0.2659\n",
      "Epoch 116/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4438 - accuracy: 0.5156 - val_loss: 4.5609 - val_accuracy: 0.2640\n",
      "Epoch 117/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4464 - accuracy: 0.5146 - val_loss: 4.5642 - val_accuracy: 0.2648\n",
      "Epoch 118/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4465 - accuracy: 0.5145 - val_loss: 4.5068 - val_accuracy: 0.2662\n",
      "Epoch 119/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4427 - accuracy: 0.5157 - val_loss: 4.6905 - val_accuracy: 0.2663\n",
      "Epoch 120/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4421 - accuracy: 0.5162 - val_loss: 4.7534 - val_accuracy: 0.2673\n",
      "Epoch 121/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4434 - accuracy: 0.5156 - val_loss: 4.7363 - val_accuracy: 0.2693\n",
      "Epoch 122/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4400 - accuracy: 0.5166 - val_loss: 4.7567 - val_accuracy: 0.2685\n",
      "Epoch 123/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4373 - accuracy: 0.5174 - val_loss: 4.7713 - val_accuracy: 0.2671\n",
      "Epoch 124/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4283 - accuracy: 0.5198 - val_loss: 4.8499 - val_accuracy: 0.2666\n",
      "Epoch 125/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4282 - accuracy: 0.5203 - val_loss: 4.8671 - val_accuracy: 0.2669\n",
      "Epoch 126/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4260 - accuracy: 0.5212 - val_loss: 4.8948 - val_accuracy: 0.2662\n",
      "Epoch 127/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4269 - accuracy: 0.5208 - val_loss: 4.8749 - val_accuracy: 0.2661\n",
      "Epoch 128/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4164 - accuracy: 0.5238 - val_loss: 4.9220 - val_accuracy: 0.2669\n",
      "Epoch 129/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4040 - accuracy: 0.5272 - val_loss: 5.0189 - val_accuracy: 0.2656\n",
      "Epoch 130/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4023 - accuracy: 0.5278 - val_loss: 5.0450 - val_accuracy: 0.2645\n",
      "Epoch 131/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4012 - accuracy: 0.5281 - val_loss: 5.1078 - val_accuracy: 0.2653\n",
      "Epoch 132/200\n",
      "137/137 [==============================] - 36s 260ms/step - loss: 1.4055 - accuracy: 0.5264 - val_loss: 5.1540 - val_accuracy: 0.2669\n",
      "Epoch 133/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4129 - accuracy: 0.5242 - val_loss: 5.0939 - val_accuracy: 0.2687\n",
      "Epoch 134/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4188 - accuracy: 0.5226 - val_loss: 5.0053 - val_accuracy: 0.2679\n",
      "Epoch 135/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4182 - accuracy: 0.5230 - val_loss: 4.9147 - val_accuracy: 0.2682\n",
      "Epoch 136/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4249 - accuracy: 0.5211 - val_loss: 4.9322 - val_accuracy: 0.2693\n",
      "Epoch 137/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4215 - accuracy: 0.5221 - val_loss: 5.0151 - val_accuracy: 0.2691\n",
      "Epoch 138/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4113 - accuracy: 0.5250 - val_loss: 5.0260 - val_accuracy: 0.2682\n",
      "Epoch 139/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4260 - accuracy: 0.5206 - val_loss: 5.0259 - val_accuracy: 0.2695\n",
      "Epoch 140/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4177 - accuracy: 0.5230 - val_loss: 5.0020 - val_accuracy: 0.2700\n",
      "Epoch 141/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4241 - accuracy: 0.5210 - val_loss: 4.9251 - val_accuracy: 0.2689\n",
      "Epoch 142/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4349 - accuracy: 0.5182 - val_loss: 4.8549 - val_accuracy: 0.2724\n",
      "Epoch 143/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4374 - accuracy: 0.5175 - val_loss: 4.8004 - val_accuracy: 0.2719\n",
      "Epoch 144/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4384 - accuracy: 0.5174 - val_loss: 4.8016 - val_accuracy: 0.2720\n",
      "Epoch 145/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4241 - accuracy: 0.5215 - val_loss: 4.7455 - val_accuracy: 0.2707\n",
      "Epoch 146/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4213 - accuracy: 0.5221 - val_loss: 4.7911 - val_accuracy: 0.2716\n",
      "Epoch 147/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4143 - accuracy: 0.5243 - val_loss: 4.8260 - val_accuracy: 0.2701\n",
      "Epoch 148/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4210 - accuracy: 0.5222 - val_loss: 4.9311 - val_accuracy: 0.2702\n",
      "Epoch 149/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4265 - accuracy: 0.5204 - val_loss: 4.8482 - val_accuracy: 0.2697\n",
      "Epoch 150/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4150 - accuracy: 0.5241 - val_loss: 4.8716 - val_accuracy: 0.2704\n",
      "Epoch 151/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4134 - accuracy: 0.5248 - val_loss: 4.8785 - val_accuracy: 0.2713\n",
      "Epoch 152/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4182 - accuracy: 0.5229 - val_loss: 4.8676 - val_accuracy: 0.2706\n",
      "Epoch 153/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4093 - accuracy: 0.5257 - val_loss: 4.8612 - val_accuracy: 0.2692\n",
      "Epoch 154/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4191 - accuracy: 0.5229 - val_loss: 4.8988 - val_accuracy: 0.2697\n",
      "Epoch 155/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4153 - accuracy: 0.5238 - val_loss: 4.9360 - val_accuracy: 0.2703\n",
      "Epoch 156/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4368 - accuracy: 0.5177 - val_loss: 4.9283 - val_accuracy: 0.2702\n",
      "Epoch 157/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4398 - accuracy: 0.5173 - val_loss: 5.0156 - val_accuracy: 0.2701\n",
      "Epoch 158/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4534 - accuracy: 0.5130 - val_loss: 4.9751 - val_accuracy: 0.2705\n",
      "Epoch 159/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4458 - accuracy: 0.5151 - val_loss: 5.0028 - val_accuracy: 0.2714\n",
      "Epoch 160/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4465 - accuracy: 0.5148 - val_loss: 5.0247 - val_accuracy: 0.2721\n",
      "Epoch 161/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4345 - accuracy: 0.5184 - val_loss: 5.0152 - val_accuracy: 0.2715\n",
      "Epoch 162/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4367 - accuracy: 0.5178 - val_loss: 5.0452 - val_accuracy: 0.2708\n",
      "Epoch 163/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4374 - accuracy: 0.5175 - val_loss: 5.0201 - val_accuracy: 0.2695\n",
      "Epoch 164/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4474 - accuracy: 0.5146 - val_loss: 4.9744 - val_accuracy: 0.2675\n",
      "Epoch 165/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4563 - accuracy: 0.5120 - val_loss: 4.9127 - val_accuracy: 0.2675\n",
      "Epoch 166/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4610 - accuracy: 0.5110 - val_loss: 4.9065 - val_accuracy: 0.2703\n",
      "Epoch 167/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4668 - accuracy: 0.5093 - val_loss: 5.0079 - val_accuracy: 0.2705\n",
      "Epoch 168/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4579 - accuracy: 0.5117 - val_loss: 5.0855 - val_accuracy: 0.2724\n",
      "Epoch 169/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4622 - accuracy: 0.5103 - val_loss: 5.0936 - val_accuracy: 0.2708\n",
      "Epoch 170/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4573 - accuracy: 0.5120 - val_loss: 4.9324 - val_accuracy: 0.2707\n",
      "Epoch 171/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4611 - accuracy: 0.5111 - val_loss: 4.9191 - val_accuracy: 0.2695\n",
      "Epoch 172/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4320 - accuracy: 0.5198 - val_loss: 5.0302 - val_accuracy: 0.2686\n",
      "Epoch 173/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4436 - accuracy: 0.5164 - val_loss: 5.1539 - val_accuracy: 0.2717\n",
      "Epoch 174/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4388 - accuracy: 0.5178 - val_loss: 5.1067 - val_accuracy: 0.2723\n",
      "Epoch 175/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4474 - accuracy: 0.5150 - val_loss: 4.9433 - val_accuracy: 0.2709\n",
      "Epoch 176/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4568 - accuracy: 0.5126 - val_loss: 4.9492 - val_accuracy: 0.2695\n",
      "Epoch 177/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4618 - accuracy: 0.5108 - val_loss: 5.0148 - val_accuracy: 0.2686\n",
      "Epoch 178/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4486 - accuracy: 0.5147 - val_loss: 5.1385 - val_accuracy: 0.2690\n",
      "Epoch 179/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4485 - accuracy: 0.5141 - val_loss: 5.2439 - val_accuracy: 0.2688\n",
      "Epoch 180/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4445 - accuracy: 0.5155 - val_loss: 5.3196 - val_accuracy: 0.2698\n",
      "Epoch 181/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4590 - accuracy: 0.5111 - val_loss: 5.2675 - val_accuracy: 0.2690\n",
      "Epoch 182/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4693 - accuracy: 0.5083 - val_loss: 5.1590 - val_accuracy: 0.2685\n",
      "Epoch 183/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4655 - accuracy: 0.5094 - val_loss: 5.0276 - val_accuracy: 0.2676\n",
      "Epoch 184/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4713 - accuracy: 0.5079 - val_loss: 5.0208 - val_accuracy: 0.2702\n",
      "Epoch 185/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4650 - accuracy: 0.5092 - val_loss: 4.9494 - val_accuracy: 0.2708\n",
      "Epoch 186/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4507 - accuracy: 0.5137 - val_loss: 5.0579 - val_accuracy: 0.2709\n",
      "Epoch 187/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4720 - accuracy: 0.5072 - val_loss: 5.0145 - val_accuracy: 0.2719\n",
      "Epoch 188/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4841 - accuracy: 0.5038 - val_loss: 4.9432 - val_accuracy: 0.2728\n",
      "Epoch 189/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4914 - accuracy: 0.5013 - val_loss: 4.8372 - val_accuracy: 0.2721\n",
      "Epoch 190/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4678 - accuracy: 0.5087 - val_loss: 4.8319 - val_accuracy: 0.2714\n",
      "Epoch 191/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4749 - accuracy: 0.5061 - val_loss: 4.8309 - val_accuracy: 0.2733\n",
      "Epoch 192/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4596 - accuracy: 0.5111 - val_loss: 4.8756 - val_accuracy: 0.2729\n",
      "Epoch 193/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4704 - accuracy: 0.5076 - val_loss: 4.9641 - val_accuracy: 0.2713\n",
      "Epoch 194/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4581 - accuracy: 0.5114 - val_loss: 5.1050 - val_accuracy: 0.2709\n",
      "Epoch 195/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4618 - accuracy: 0.5100 - val_loss: 5.1243 - val_accuracy: 0.2731\n",
      "Epoch 196/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4699 - accuracy: 0.5077 - val_loss: 5.1477 - val_accuracy: 0.2720\n",
      "Epoch 197/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4652 - accuracy: 0.5088 - val_loss: 4.9287 - val_accuracy: 0.2735\n",
      "Epoch 198/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4714 - accuracy: 0.5073 - val_loss: 5.0197 - val_accuracy: 0.2724\n",
      "Epoch 199/200\n",
      "137/137 [==============================] - 36s 263ms/step - loss: 1.4725 - accuracy: 0.5070 - val_loss: 4.9681 - val_accuracy: 0.2724\n",
      "Epoch 200/200\n",
      "137/137 [==============================] - 36s 262ms/step - loss: 1.4763 - accuracy: 0.5059 - val_loss: 5.0784 - val_accuracy: 0.2744\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    ds,\n",
    "    epochs=200,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
